{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bded86b-5cab-437c-a514-46d15ec084cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c1b65-5f69-4280-8a48-6ed3392454bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TIHMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str = \"./\",\n",
    "        train=True,\n",
    "        imputer=impute.SimpleImputer(),\n",
    "        n_days: int = 1,\n",
    "        normalise: typing.Union[str, None] = \"global\",\n",
    "    ):\n",
    "        \n",
    "\n",
    "        self.train = train  # saving whether training or testing\n",
    "        self._dataset = TIHM(root=root)  # the dataset\n",
    "\n",
    "        # splitting the data by date to get train-test split\n",
    "        train_data, test_data, train_target, test_target = self._train_test_split(\n",
    "            data=self._dataset.data, target=self._dataset.target, test_start=TEST_START\n",
    "        )\n",
    "\n",
    "        ## getting arrays from data frames\n",
    "        # train\n",
    "        train_patient_id = train_data[\"patient_id\"].values\n",
    "        train_date = train_data[\"date\"].dt.date.values\n",
    "        train_data = train_data.drop([\"patient_id\", \"date\"], axis=1).values\n",
    "        train_target = train_target.drop([\"patient_id\", \"date\"], axis=1).values\n",
    "        # test\n",
    "        test_patient_id = test_data[\"patient_id\"].values\n",
    "        test_date = test_data[\"date\"].dt.date.values\n",
    "        test_data = test_data.drop([\"patient_id\", \"date\"], axis=1).values\n",
    "        test_target = test_target.drop([\"patient_id\", \"date\"], axis=1).values\n",
    "\n",
    "        # impute the data with the given imputer\n",
    "        train_data, test_data = self._impute(\n",
    "            train_data=train_data, test_data=test_data, imputer=imputer\n",
    "        )\n",
    "\n",
    "        if not normalise is None:\n",
    "            # scale the data with the sklearn StandardScaler\n",
    "            train_data, test_data = self._normalise(\n",
    "                train_data=train_data,\n",
    "                test_data=test_data,\n",
    "                train_patient_id=train_patient_id,\n",
    "                test_patient_id=test_patient_id,\n",
    "                normalise=normalise,\n",
    "            )\n",
    "\n",
    "        # reformatting the training and testing data to contain the n_days in each data point\n",
    "        if n_days > 1:\n",
    "            (\n",
    "                train_data,\n",
    "                train_target,\n",
    "                train_patient_id,\n",
    "                train_date,\n",
    "            ) = self._reformat_n_days(\n",
    "                data=train_data,\n",
    "                target=train_target,\n",
    "                patient_id=train_patient_id,\n",
    "                date=train_date,\n",
    "                n_days=n_days,\n",
    "            )\n",
    "\n",
    "            test_data, test_target, test_patient_id, test_date = self._reformat_n_days(\n",
    "                data=test_data,\n",
    "                target=test_target,\n",
    "                patient_id=test_patient_id,\n",
    "                date=test_date,\n",
    "                n_days=n_days,\n",
    "            )\n",
    "\n",
    "        # saving data to class attributes\n",
    "        self.train_data, self.test_data = train_data, test_data\n",
    "        self.train_target, self.test_target = train_target, test_target\n",
    "        self.train_patient_id, self.test_patient_id = train_patient_id, test_patient_id\n",
    "        self.train_date, self.test_date = train_date, test_date\n",
    "\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def feature_names(self) -> typing.List[str]:\n",
    "        \"\"\"\n",
    "        The names of the features in the x data.\n",
    "        \"\"\"\n",
    "        return list(self._dataset.data.drop([\"patient_id\", \"date\"], axis=1).columns)\n",
    "\n",
    "    @property\n",
    "    def target_names(self) -> typing.List[str]:\n",
    "        \"\"\"\n",
    "        The names of the features in the y data.\n",
    "        \"\"\"\n",
    "        return list(self._dataset.target.drop([\"patient_id\", \"date\"], axis=1).columns)\n",
    "\n",
    "    def _train_test_split(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        target: pd.DataFrame,\n",
    "        test_start: str,\n",
    "    ) -> typing.Tuple[pd.DataFrame]:\n",
    "\n",
    "        # train\n",
    "        train_data = data[data[\"date\"] < pd.to_datetime(test_start)]\n",
    "        train_target = target[target[\"date\"] < pd.to_datetime(test_start)]\n",
    "\n",
    "        # test\n",
    "        test_data = data[data[\"date\"] >= pd.to_datetime(test_start)]\n",
    "        test_target = target[target[\"date\"] >= pd.to_datetime(test_start)]\n",
    "\n",
    "        return train_data, test_data, train_target, test_target\n",
    "\n",
    "    def _impute(\n",
    "        self, train_data: np.ndarray, test_data: np.ndarray, imputer\n",
    "    ) -> typing.Tuple[np.ndarray]:\n",
    "\n",
    "        try:\n",
    "            train_data = imputer.fit_transform(\n",
    "                train_data\n",
    "            )  # fit and transform with the train data\n",
    "            test_data = imputer.transform(test_data)  # transform with the test data\n",
    "        except AttributeError:\n",
    "            raise TypeError(\n",
    "                \"Please ensure that the imputer is a sklearn imputer, \"\n",
    "                + \"or implements the fit_transform and transform methods.\"\n",
    "            )\n",
    "        return train_data, test_data\n",
    "\n",
    "    def _normalise(\n",
    "        self,\n",
    "        train_data: np.ndarray,\n",
    "        test_data: np.ndarray,\n",
    "        train_patient_id: np.ndarray,\n",
    "        test_patient_id: np.ndarray,\n",
    "        normalise: str,\n",
    "    ) -> typing.Tuple[np.ndarray]:\n",
    "\n",
    "        if normalise == \"global\":\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            train_data = scaler.fit_transform(train_data)\n",
    "            test_data = scaler.transform(test_data)\n",
    "\n",
    "        elif normalise == \"id\":\n",
    "            scaler = StandardGroupScaler()\n",
    "            train_data = scaler.fit_transform(train_data, groups=train_patient_id)\n",
    "            test_data = scaler.transform(test_data, groups=test_patient_id)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"normalise must be None, 'global' or 'id', not {normalise}.\"\n",
    "            )\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "    def _reformat_n_days(\n",
    "        self,\n",
    "        data: np.ndarray,\n",
    "        target: np.ndarray,\n",
    "        patient_id: np.ndarray,\n",
    "        date: np.ndarray,\n",
    "        n_days: int,\n",
    "    ) -> typing.Tuple[np.ndarray]:\n",
    "\n",
    "        # new arrays\n",
    "        data_out = []\n",
    "        target_out = []\n",
    "        patient_id_out = []\n",
    "        date_out = []\n",
    "\n",
    "        # iterate over patient_ids\n",
    "        for n_id, id_val in enumerate(np.unique(patient_id)):\n",
    "            idx_id = np.arange(data.shape[0])[patient_id == id_val][\n",
    "                np.argsort(date[patient_id == id_val])\n",
    "            ]\n",
    "            idx_split = np.where(\n",
    "                date[idx_id][1:] - date[idx_id][:-1] > dt.timedelta(days=1)\n",
    "            )[0]\n",
    "            idx_split = np.split(np.arange(idx_id.shape[0]), idx_split + 1)\n",
    "\n",
    "            for n_split, i_split in enumerate(idx_split):\n",
    "                data_i = data[idx_id][i_split]\n",
    "                target_i = target[idx_id][i_split]\n",
    "                patient_id_i = patient_id[idx_id][i_split]\n",
    "                date_i = date[idx_id][i_split]\n",
    "\n",
    "                # if X_i is not long enough to build a sequence\n",
    "                # then we skip it\n",
    "                if data_i.shape[0] < n_days:\n",
    "                    continue\n",
    "\n",
    "                # roll the data\n",
    "                data_i_rolled = make_input_roll(data_i, n_days)\n",
    "                target_i_rolled = make_input_roll(target_i, n_days)\n",
    "                patient_id_i_rolled = make_input_roll(patient_id_i, n_days)\n",
    "                date_i_rolled = make_input_roll(date_i, n_days)\n",
    "\n",
    "                # append to the outputs\n",
    "                data_out.append(data_i_rolled)\n",
    "                target_out.append(target_i_rolled)\n",
    "                patient_id_out.append(patient_id_i_rolled)\n",
    "                date_out.append(date_i_rolled)\n",
    "\n",
    "        # make outputs arrays\n",
    "        data_out = np.vstack(data_out)\n",
    "        target_out = np.vstack(target_out)\n",
    "        patient_id_out = np.vstack(patient_id_out)\n",
    "        date_out = np.vstack(date_out)\n",
    "\n",
    "        return data_out, target_out, patient_id_out, date_out\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if self.train:\n",
    "            x, y = self.train_data[index], self.train_target[index]\n",
    "        else:\n",
    "            x, y = self.test_data[index], self.test_target[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data) if self.train else len(self.test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
